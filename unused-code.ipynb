{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# JuMP.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ncmjump(U, H; method=:PG, accelerate=true, τ=1.0, kmax=30, verbose=true)\n",
    "    \n",
    "    issymmetric(U) || error(\"U must be symmetric\")\n",
    "    issymmetric(H) || error(\"H must be symmetric\")\n",
    "    \n",
    "    n = size(U, 1)\n",
    "    H2 = Symmetric(H.*H)\n",
    "    \n",
    "    M = Symmetric(zeros(n,n))\n",
    "    \n",
    "    # Loss function and gradient\n",
    "    f(X) = 0.5*norm(M.data .= H.*(X .- U))^2\n",
    "    ∇f(X) = Symmetric(H2.*(X .- U))\n",
    "\n",
    "    # Lipschitz constant of ∇f\n",
    "    L = norm(H2)\n",
    "\n",
    "    # Quadratic model of f at Y\n",
    "    Q(X,Y,L,τ) = f(Y) + dot(∇f(Y), X - Y) + L/2τ*dot(X - Y, X - Y)\n",
    "\n",
    "    # Initialize model with correlation matrix constraints\n",
    "    m = Model(with_optimizer(CSDP.Optimizer))\n",
    "    #m = Model(with_optimizer(COSMO.Optimizer))\n",
    "    MOI.set(m, MOI.Silent(), true)\n",
    "    @variable(m, X[1:n,1:n], Symmetric)\n",
    "    @constraint(m, diagcon, diag(X) .== 1)\n",
    "    @constraint(m, psdcon, X in PSDCone())\n",
    "\n",
    "    #X0 = Symmetric(diagm(ones(n)))\n",
    "    X0 = Symmetric(zeros(n,n))\n",
    "    fvals = [f(X0)]\n",
    "    \n",
    "    y = zeros(n)\n",
    "    Λ = Symmetric(zeros(n,n))\n",
    "    Γ = copy(Λ)\n",
    "    V = copy(Λ)\n",
    "    ∇fY = copy(Λ)\n",
    "    Y = copy(X0)\n",
    "    Xold = copy(X0)\n",
    "    Xnew = copy(X0)\n",
    "    t = 1.0\n",
    "    \n",
    "    for k = 1:kmax\n",
    "        \n",
    "        # Solve quadratic subproblem\n",
    "        @objective(m, MOI.FEASIBILITY_SENSE, 0)\n",
    "        @objective(m, Min, Q(X,Y,L,τ))\n",
    "        optimize!(m)\n",
    "        Xnew.data .= value.(X)\n",
    "\n",
    "        # Ensure that diag(Xnew).==1 exactly\n",
    "        d = diag(Xnew)\n",
    "        for i=1:n\n",
    "            d[i] = 1/sqrt(Xnew[i,i])\n",
    "        end\n",
    "        for j=1:n\n",
    "            for i=1:n\n",
    "                Xnew.data[i,j] = d[i]*d[j]*Xnew.data[i,j]\n",
    "            end\n",
    "        end\n",
    "                \n",
    "        # Evaluate loss function\n",
    "        fnew = f(Xnew)\n",
    "        Qnew = Q(Xnew,Y,L,τ)\n",
    "        push!(fvals, fnew)\n",
    "\n",
    "        # Obtain dual multipliers\n",
    "        y .= dual.(diagcon)\n",
    "        copy!(Λ, dual.(psdcon))\n",
    "        \n",
    "        # Compute subgradient Γ ∈ ∂g(X; ε)\n",
    "        Γ.data .= -Λ .- Diagonal(y)\n",
    "\n",
    "        # Compute dual residual\n",
    "        ∇fY.data .= H2.*(Y .- U)\n",
    "        V.data .= ∇fY .+ (L/τ).*(Xnew .- Y) .+ Γ\n",
    "\n",
    "        for i=1:n\n",
    "            d[i] = 1 - Xnew[i,i]\n",
    "        end\n",
    "        Rp = norm(d)/(1 + √n)\n",
    "        Rd = norm(M.data .= H2.*(Xnew .- U) .- Diagonal(y) .- Λ)\n",
    "        \n",
    "        ε = dot(Xnew, Λ)\n",
    "        δ = norm(V)\n",
    "        dist = norm(M.data .= Xnew .- Y)\n",
    "        \n",
    "        if verbose\n",
    "            mod(k, 20)==1 &&\n",
    "            @printf(\"%4s %10s %10s %10s %10s %10s %10s\\n\", \n",
    "                \"k\", \"f(X)\", \"Rp\", \"Rd\", \"<X,Λ>\", \"||V||\", \"||X-Y||\")\n",
    "            @printf(\"%4d %10.2e %10.2e %10.2e %10.2e %10.2e %10.2e\\n\", \n",
    "                k, fvals[end], Rp, Rd, ε, δ, dist)\n",
    "        end\n",
    "        \n",
    "        #=\n",
    "        if method==:IR\n",
    "            (τ*δ)^2 + 2τ*ε*L ≤ (1-τ)*(L*dist)^2 || \n",
    "                @warn(\"(τ*δ)^2 + 2τ*ε*L ≤ (1-τ)*(L*dist)^2 fails\")\n",
    "        end\n",
    "        =#\n",
    "\n",
    "        #=\n",
    "        if verbose\n",
    "            mod(k, 20)==1 &&\n",
    "            @printf(\"%4s %8s %8s %10s %10s %10s %6s\\n\", \n",
    "                \"k\", \"ε\", \"δ\", \"||X - Y||\", \"f(X)\", \"Q(X, Y)\", \"t\")\n",
    "            @printf(\"%4d %8.1e %8.1e %10.1e %10.2e %10.2e %6.2f\\n\", \n",
    "                k, ε, δ, dist, fnew, Qnew, t)\n",
    "        end\n",
    "        =#\n",
    "\n",
    "        # Update Y, Xold, t\n",
    "        if accelerate\n",
    "            tnew = (1 + √(1 + 4t^2))/2\n",
    "        else\n",
    "            tnew = 1.0\n",
    "        end\n",
    "        \n",
    "        if method==:IR\n",
    "            Y.data .= Xnew .- (t/tnew*τ/L).*V .+ ((t - 1)/tnew).*(Xnew .- Xold)\n",
    "        else\n",
    "            Y.data .= Xnew .+ ((t - 1)/tnew).*(Xnew .- Xold)\n",
    "        end\n",
    "        Xold .= Xnew\n",
    "        t = tnew\n",
    "    end\n",
    "    \n",
    "    return Xnew, y, fvals\n",
    "end\n",
    "\n",
    "n = 6\n",
    "U, H = randncm(n)\n",
    "ncmjump(U, H, kmax=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "U, H = randncm(n)\n",
    "kmax = 50\n",
    "verbose = false\n",
    "\n",
    "@time X0, y0, fvals0 = ncmjump(U, H, accelerate=false,  kmax=kmax, verbose=verbose)\n",
    "@time X,  y,  fvals  = ncmjump(U, H, accelerate=true,   kmax=kmax, verbose=verbose)\n",
    "@time X1, y1, fvals1 = ncmjump(U, H, method=:IR, τ=0.1, kmax=kmax, verbose=verbose)\n",
    "@time X2, y2, fvals2 = ncmjump(U, H, method=:IR, τ=0.5, kmax=kmax, verbose=verbose)\n",
    "@time X3, y3, fvals3 = ncmjump(U, H, method=:IR, τ=0.9, kmax=kmax, verbose=verbose)\n",
    "\n",
    "plot(yaxis=:log, legend=:topright, size=(900,600), title=\"subproblem solver: JuMP with CSDP\")\n",
    "plot!(0:kmax, fvals0, label=\"PG\", linestyle=:auto)\n",
    "plot!(0:kmax, fvals, label=\"APG\", linestyle=:auto)\n",
    "plot!(0:kmax, fvals1, label=L\"\\tau = 0.1\", markershape=:auto)\n",
    "plot!(0:kmax, fvals2, label=L\"\\tau = 0.5\", markershape=:auto)\n",
    "plot!(0:kmax, fvals3, label=L\"\\tau = 0.9\", markershape=:auto)\n",
    "xlabel!(L\"k\"); ylabel!(L\"f(x_k)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Optim.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ncm2(U, H, myproj; \n",
    "        τ=1.0, \n",
    "        t0=1.0,\n",
    "        f_calls_limit=1000, \n",
    "        kmax=300,\n",
    "        tol=1e-2,\n",
    "        g_tol=1e-2, \n",
    "        algo=GradientDescent(), \n",
    "        method=:PG, \n",
    "        accelerate=true, \n",
    "        verbose=true,\n",
    "    )\n",
    "        \n",
    "    # Check for valid input\n",
    "    issymmetric(U) || error(\"U must be symmetric\")\n",
    "    issymmetric(H) || error(\"H must be symmetric\")\n",
    "    size(U) == size(H) || error(\"U and H must be the same size\")\n",
    "    \n",
    "    # Create the projection function\n",
    "    n = size(U, 1)\n",
    "    \n",
    "    H2 = Symmetric(H.*H)\n",
    "    \n",
    "    # Loss function and gradient\n",
    "    f(X) = norm(H.*(X .- U))\n",
    "    #∇f(X) = Symmetric(H2.*(X .- U))\n",
    "    \n",
    "    # Lipschitz constant of ∇f\n",
    "    L = norm(H2)\n",
    "    \n",
    "    # Memory allocation\n",
    "    y = zeros(n)\n",
    "    g = zeros(n)\n",
    "    d = zeros(n)\n",
    "    M = Symmetric(zeros(n,n))\n",
    "    #Y = Symmetric(triu(zeros(n,n),1) + I)\n",
    "    Y = copy(M)\n",
    "    Xold = copy(M)\n",
    "    Xnew = copy(M)\n",
    "    Λ = copy(M)\n",
    "    Γ = copy(M)\n",
    "    V = copy(M)\n",
    "    X = copy(M)\n",
    "    ∇fY = copy(M)\n",
    "    fvals = Float64[]\n",
    "    \n",
    "    # Evaluates dual objective function and its gradient\n",
    "    function dualobj!(ff, gg, y)\n",
    "        ∇fY.data .= H2.*(Y .- U)\n",
    "        M.data .= Y .- (τ/L).*(∇fY .+ Diagonal(y))\n",
    "        X .= M\n",
    "        myproj(X)\n",
    "        \n",
    "        # Update Λ and Γ\n",
    "        Λ.data .= (L/τ).*(X .- M)\n",
    "        Γ.data .= Diagonal(y) .- Λ\n",
    "        \n",
    "        # Ensure that diag(Xnew).==1 exactly\n",
    "        for i=1:n\n",
    "            d[i] = 1/sqrt(X[i,i])\n",
    "        end\n",
    "        for j=1:n\n",
    "            for i=1:n\n",
    "                Xnew.data[i,j] = d[i]*d[j]*X[i,j]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "         M.data .= H.*(Xnew .- U)\n",
    "        push!(fvals, 0.5*dot(M,M))\n",
    "\n",
    "        V.data .= ∇fY .+ (L/τ).*(Xnew .- Y) .+ Γ\n",
    "\n",
    "        if gg != nothing\n",
    "            for i=1:n\n",
    "                gg[i] = 1 - X[i,i]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if ff != nothing\n",
    "            normW = norm(view(myproj.w, 1:myproj.m[]))\n",
    "            return sum(y) + (L/2τ)*(normW^2 - dot(Y,Y))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # The callback function\n",
    "    function cb(os)\n",
    "        #println(\"================ CALLBACK ================\")\n",
    "        #@show os[end].metadata[\"x\"]\n",
    "        \n",
    "        # Compute ε and δ\n",
    "        ε = max(0.0, dot(Xnew, Λ))\n",
    "        δ = norm(V)\n",
    "        dist = norm(M.data .= Xnew .- Y)\n",
    "        \n",
    "        stop_optimize = false\n",
    "        if (τ*δ)^2 + 2τ*ε*L ≤ (1-τ)*(L*dist)^2\n",
    "            #@show (τ*δ)^2 + 2τ*ε*L, (1-τ)*(L*dist)^2\n",
    "            stop_optimize = true\n",
    "        end\n",
    "        #if length(fvals) >= f_calls_limit\n",
    "        #    stop_optimize = true\n",
    "        #end\n",
    "        #println(\"==========================================\")\n",
    "        return stop_optimize\n",
    "    end\n",
    "    \n",
    "    # Define subproblem\n",
    "    prob = Optim.only_fg!(dualobj!)\n",
    "    opts = Optim.Options(\n",
    "            g_tol=g_tol, \n",
    "            store_trace=true, \n",
    "            extended_trace=true, \n",
    "            show_trace=false, \n",
    "            callback=cb)\n",
    "    \n",
    "    Rp = Rd = Inf\n",
    "    k = 0\n",
    "    t = t0\n",
    "    while max(Rp, Rd) > tol && k < kmax && length(fvals) < f_calls_limit\n",
    "        k = k + 1\n",
    "        \n",
    "        res = optimize(prob, y, algo, opts)\n",
    "        \n",
    "        #if length(fvals) < f_calls_limit && !Optim.g_converged(res)\n",
    "        #    error(\"Failed to solve subproblem: gnorm ≰ g_tol\")\n",
    "        #end\n",
    "        \n",
    "        gnorm = Optim.g_norm_trace(res)[end]\n",
    "        \n",
    "        y .= res.minimizer\n",
    "        \n",
    "        # Compute ε and δ\n",
    "        ε = dot(Xnew, Λ)\n",
    "        δ = norm(V)\n",
    "        dist = norm(M.data .= Xnew .- Y)\n",
    "        \n",
    "        for i=1:n\n",
    "            d[i] = 1 - Xnew[i,i]\n",
    "        end\n",
    "        Rp = norm(d)/(1 + √n)\n",
    "        Rd = norm(M.data .= H2.*(Xnew .- U) .- Diagonal(y) .- Λ)\n",
    "        \n",
    "        if verbose\n",
    "            mod(k, 20)==1 &&\n",
    "            @printf(\"%4s %8s %8s %10s %10s %10s %10s %10s %10s %10s %10s\\n\", \n",
    "                \"k\", \"f_calls\", \"g_calls\", \"||g||\", \"g_tol\", \"f(X)\", \"Rp\", \"Rd\", \"<X,Λ>\", \"||V||\", \"||X-Y||\")\n",
    "            @printf(\"%4d %8d %8d %10.2e %10.2e %10.2e %10.2e %10.2e %10.2e %10.2e %10.2e\\n\", \n",
    "                k, res.f_calls, res.g_calls, gnorm, g_tol, fvals[end], Rp, Rd, ε, δ, dist)\n",
    "        end\n",
    "        \n",
    "        #=\n",
    "        if (τ*δ)^2 + 2τ*ε*L > (1-τ)*(L*dist)^2 \n",
    "            println(\"WARNING: (τ*δ)^2 + 2τ*ε*L ≤ (1-τ)*(L*dist)^2 fails\")\n",
    "            @show (τ*δ)^2 + 2τ*ε*L, (1-τ)*(L*dist)^2\n",
    "        end\n",
    "        =#\n",
    "\n",
    "        # Update Y, Xold, t\n",
    "        tnew = (1 + √(1 + 4t^2))/2\n",
    "        Y.data .= Xnew .- (t/tnew*τ/L).*V .+ ((t - 1)/tnew).*(Xnew .- Xold)\n",
    "        Xold .= Xnew\n",
    "        t = tnew\n",
    "    end\n",
    "    #@show length(fvals), fvals[end]\n",
    "    \n",
    "    return Xnew, y, fvals\n",
    "end\n",
    "\n",
    "n = 6\n",
    "myproj = ProjPSD(n)\n",
    "U, H = randncm(n)\n",
    "@time X, y, fvals = ncm2(U, H, myproj, \n",
    "    algo=BFGS(linesearch=HagerZhang()), \n",
    "    g_tol=1e-4, \n",
    "    τ=1.0, \n",
    "    f_calls_limit=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#InitialStatic()\n",
    "#InitialPrevious()\n",
    "#InitialQuadratic()\n",
    "#InitialConstantChange(αmin = 1e-12, αmax = 1.0, α0 = 1.0, ρ = 0.25, snap2one = (0.75, Inf))\n",
    "#InitialHagerZhang()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "myproj = ProjPSD(n)\n",
    "U, H = randncm(n)\n",
    "X = copy(U)\n",
    "\n",
    "f_calls_limit=1000\n",
    "tol=1e-1\n",
    "\n",
    "alphaguess=InitialStatic(alpha=1.0)\n",
    "\n",
    "plt = plot(yaxis=:log, legend=:topright, size=(900,600), title=\"subproblem solver: Optim.jl\")\n",
    "for algo in [BFGS] #[BFGS, LBFGS, ConjugateGradient, GradientDescent]\n",
    "    for ls in [BackTracking] #[BackTracking, StrongWolfe, HagerZhang, MoreThuente, Static]\n",
    "        for g_tol in [0.0]\n",
    "            for τ in 0.85:0.05:0.95\n",
    "                label=\"\\\\tau=$τ\"\n",
    "                println(label)\n",
    "                @time X, y, fvals = ncm2(U, H, myproj,\n",
    "                    τ=τ, \n",
    "                    f_calls_limit=f_calls_limit, \n",
    "                    tol=tol,\n",
    "                    g_tol=g_tol, \n",
    "                    algo=algo(linesearch=ls(), alphaguess=alphaguess), \n",
    "                    verbose=false,\n",
    "                )\n",
    "                plot!(1:length(fvals), fvals, label=label)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "xlabel!(\"function evaluations\"); ylabel!(\"objective function value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LineSearches.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "U, H = randncm(n)\n",
    "\n",
    "H2 = Symmetric(H.*H)\n",
    "    \n",
    "# Loss function and gradient\n",
    "f(X) = 0.5*norm(H.*(X .- U))^2\n",
    "∇f(X) = Symmetric(H2.*(X .- U))\n",
    "\n",
    "# Lipschitz constant of ∇f\n",
    "L = norm(H2)\n",
    "\n",
    "k = 0\n",
    "τ = 1.0\n",
    "α = 1.0\n",
    "ff = 0.0\n",
    "g = zeros(n)\n",
    "d = similar(g)\n",
    "y = zeros(n)\n",
    "M = Symmetric(zeros(n,n))\n",
    "gg = zeros(n)\n",
    "Y = copy(M)\n",
    "\n",
    "myproj = ProjPSD(n)\n",
    "\n",
    "function dualobj!(ff, gg, y, M)\n",
    "    copy!(M.data, Y .- (τ/L).*(∇f(Y) .+ Diagonal(y)))\n",
    "    myproj(M)\n",
    "    if gg != nothing\n",
    "        copy!(gg, 1 .- diag(M))\n",
    "    end\n",
    "    if ff != nothing\n",
    "        normW = norm(myproj.w[1:myproj.m[]])\n",
    "        return sum(y) + (L/2τ)*(normW^2 - dot(Y,Y))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k += 1\n",
    "\n",
    "dobj = dualobj!(ff, g, y, M)\n",
    "d .= -g\n",
    "@show norm(g)\n",
    "\n",
    "ϕ(t) = dualobj!(ff, nothing, y.+t.*d, M)\n",
    "function dϕ(t)\n",
    "    dualobj!(nothing, gg, y.+t.*d, M)\n",
    "    return dot(d, gg)\n",
    "end\n",
    "ϕdϕ(t) = ϕ(t), dϕ(t)\n",
    "\n",
    "α0 = α\n",
    "ϕ0, dϕ0 = ϕdϕ(0.0)\n",
    "α, ϕα = StrongWolfe()(ϕ, dϕ, ϕdϕ, α0, ϕ0, dϕ0)\n",
    "\n",
    "xticks=[0.0, α, α0, 2α0]\n",
    "yticks=[ϕ0, ϕ(α), ϕ(α0)]\n",
    "\n",
    "plt = plot(ϕ, 0.0, α+10.0, \n",
    "    c=:blue, title=\"Iter. $k\", legend=false, xticks=xticks, yticks=yticks)\n",
    "plot!(t -> ϕ0 + t*dϕ0, 0.0, α+5.0, c=:red, s=:dash)\n",
    "plot!(t -> ϕ(α) + (t-α)*dϕ(α), α, α+5.0, c=:black, s=:dash)\n",
    "scatter!([0.0], [ϕ0], c=:red, ms=3)\n",
    "scatter!([α], [ϕ(α)], c=:black, ms=3)\n",
    "display(plt)\n",
    "\n",
    "y .+= α.*d;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
